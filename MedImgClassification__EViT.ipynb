{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "gdgYIhXKe40R"
      ],
      "authorship_tag": "ABX9TyNYrO/cq6TNtGROKbCy/iws",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TienTranTrung/PlantMedResearch/blob/master/MedImgClassification__EViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install library and import dataset"
      ],
      "metadata": {
        "id": "xsMA7vb-6CJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and Install Dependencies\n",
        "!pip install -q numpy timm pretrainedmodels gdown==5.1.0\n",
        "!gdown --id 1k4I5_GUmOcPuxdz_L027xhFZCwibAgiY\n",
        "!pip install -q pytorch-lightning torchvision albumentations grad-cam\n",
        "# !git clone https://github.com/TienTranTrung/ASL_Rerverse_translate.git\n",
        "# !pip install -q pretrainedmodels"
      ],
      "metadata": {
        "id": "qEYBPAtBIX05",
        "outputId": "236d0e14-a71b-4a22-e097-14c4f45ed0b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1k4I5_GUmOcPuxdz_L027xhFZCwibAgiY\n",
            "From (redirected): https://drive.google.com/uc?id=1k4I5_GUmOcPuxdz_L027xhFZCwibAgiY&confirm=t&uuid=d4979e55-f8de-4783-b6b6-790d6e4aab13\n",
            "To: /content/traditional_medicine_dataset.zip\n",
            "100% 216M/216M [00:02<00:00, 98.4MB/s]\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.2/802.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!unzip -q 'traditional_medicine_dataset' -d data"
      ],
      "metadata": {
        "id": "ptjF36kMHLuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import v2 as T\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.optim import Adam\n",
        "import os\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from matplotlib.colors import LogNorm"
      ],
      "metadata": {
        "id": "jURMkmpMgHQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations\n",
        "train_transform = T.Compose([\n",
        "    T.RandomResizedCrop(224),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomRotation(90),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    # Comment out CutMix and other complex transforms for now\n",
        "    # T.RandomApply([T.RandomErasing(p=1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random')], p=0.5),\n",
        "    # T.RandomApply([T.CutMix(alpha=1.0, num_classes=200)], p=0.5)\n",
        "])\n",
        "\n",
        "val_transform = T.Compose([\n",
        "    T.Resize(256),\n",
        "    T.CenterCrop(224),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])"
      ],
      "metadata": {
        "id": "p38uPx_CiN4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d353d6-8bc2-431b-da4d-3844726bfe71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class to apply Albumentations transformations\n",
        "class VMedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.dataset[idx]\n",
        "        image = np.array(image)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        # Debug: check the type and shape of the image\n",
        "        if not isinstance(image, torch.Tensor):\n",
        "            raise TypeError(f\"Transformed image is not a tensor, got {type(image)}\")\n",
        "        return image, label\n",
        "\n",
        "data_dir = 'data/Dataset'\n",
        "\n",
        "# Create datasets\n",
        "image_datasets = {x: ImageFolder(os.path.join(data_dir, x)) for x in ['train', 'test']}\n",
        "train_size = int(0.8 * len(image_datasets['train']))\n",
        "val_size = len(image_datasets['train']) - train_size\n",
        "train_dataset, val_dataset = random_split(image_datasets['train'], [train_size, val_size])\n",
        "train_dataset = VMedDataset(train_dataset, transform=val_transform)\n",
        "val_dataset = VMedDataset(val_dataset, transform=val_transform)\n",
        "test_dataset = VMedDataset(image_datasets['test'], transform=val_transform)\n",
        "\n",
        "dataloaders = {\n",
        "    'train': DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4),\n",
        "    'val': DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=4),\n",
        "    'test': DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "}\n",
        "dataset_sizes = {x: len(dataloaders[x].dataset) for x in ['train', 'val', 'test']}\n",
        "for images, labels in dataloaders['train']:\n",
        "    print(f'Image batch shape: {images.size()}')\n",
        "    print(f'Label batch shape: {labels.size()}')\n",
        "    break\n",
        "class_names = image_datasets['train'].classes"
      ],
      "metadata": {
        "id": "IYzD1jVliVpH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8997f24d-9fa7-4265-eab2-917899f78b27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch shape: torch.Size([32, 3, 128, 128])\n",
            "Label batch shape: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom model"
      ],
      "metadata": {
        "id": "gdgYIhXKe40R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(pl.LightningModule):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(128 * 56 * 56, 512)  # Adjust this according to the output size after conv2\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs)\n",
        "        val_loss = F.cross_entropy(outputs, labels)\n",
        "        self.log('val_loss', val_loss)\n",
        "        return val_loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs)\n",
        "        test_loss = F.cross_entropy(outputs, labels)\n",
        "        self.log('test_loss', test_loss)\n",
        "        return test_loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
        "        return optimizer\n",
        "\n",
        "num_classes = len(class_names)\n",
        "model = SimpleCNN(num_classes)"
      ],
      "metadata": {
        "id": "9kCCxdXni91M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mobilenet"
      ],
      "metadata": {
        "id": "3EKTLWdBe9MI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MobileNetV2(pl.LightningModule):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MobileNetV2, self).__init__()\n",
        "        self.model = models.mobilenet_v2(pretrained=True, **kwargs)\n",
        "        self.model.classifier = nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model.features(x)\n",
        "        return x\n",
        "\n",
        "class LocationWiseSoftAttentionModule(pl.LightningModule):\n",
        "    def __init__(self, in_channels):\n",
        "        super(LocationWiseSoftAttentionModule, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
        "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
        "        self.value_conv = nn.Conv2d(in_channels, in_channels, 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        proj_query = self.query_conv(x).view(x.shape[0], -1, x.shape[2]*x.shape[3]).permute(0, 2, 1)\n",
        "        proj_key = self.key_conv(x).view(x.shape[0], -1, x.shape[2]*x.shape[3])\n",
        "        attention_scores = torch.bmm(proj_query, proj_key)\n",
        "        attention_map = torch.softmax(attention_scores, dim=-1)\n",
        "        proj_value = self.value_conv(x).view(x.shape[0], x.shape[2]*x.shape[3], -1)\n",
        "        weighted = torch.bmm(attention_map, proj_value).view(x.shape)\n",
        "        return x + self.gamma * weighted\n",
        "\n",
        "class DepthwiseSeparableConv(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1):\n",
        "        super(DepthwiseSeparableConv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class MedPlantClassifier(pl.LightningModule):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MedPlantClassifier, self).__init__()\n",
        "        self.mobilenetv2 = MobileNetV2()\n",
        "\n",
        "        self.lsam = LocationWiseSoftAttentionModule(in_channels=1280)\n",
        "        self.conv1 = DepthwiseSeparableConv(1280, 256, 3, padding=1)  # Ensure padding is sufficient\n",
        "        self.conv2 = DepthwiseSeparableConv(256, 128, 3, padding=1)\n",
        "        self.classifier = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mobilenetv2(x)\n",
        "        x = self.lsam(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.mean([2, 3])\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs)\n",
        "        val_loss = F.cross_entropy(outputs, labels)\n",
        "        self.log('val_loss', val_loss)\n",
        "        return val_loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs)\n",
        "        test_loss = F.cross_entropy(outputs, labels)\n",
        "        self.log('test_loss', test_loss)\n",
        "        return test_loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = Adam(self.parameters(), lr=0.001)\n",
        "        return optimizer\n",
        "num_classes = len(class_names)\n",
        "model = MedPlantClassifier(num_classes)"
      ],
      "metadata": {
        "id": "jwUkQ7agKQhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks for checkpointing and early stopping\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_loss',\n",
        "    dirpath='checkpoints',\n",
        "    filename='best-checkpoint',\n",
        "    save_top_k=1,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    verbose=True,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "# Training the model\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=20,\n",
        "    # gpus=1 if torch.cuda.is_available() else 0,\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
        ")\n",
        "\n",
        "trainer.fit(model, dataloaders['train'], dataloaders['val'])"
      ],
      "metadata": {
        "id": "dyPMiqe9qm9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model for testing\n",
        "best_model_path = checkpoint_callback.best_model_path\n",
        "trained_model = MedPlantClassifier.load_from_checkpoint(best_model_path, num_classes=num_classes)\n",
        "\n",
        "# Test the model\n",
        "trainer.test(trained_model, dataloaders['test'])\n",
        "\n",
        "# Visualize the results\n",
        "def plot_results(model, dataloader, class_names, top_n_classes=20):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(model.device), labels.to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Display the full confusion matrix\n",
        "    fig, ax = plt.subplots(figsize=(20, 20))  # Adjusted size\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    disp.plot(ax=ax, xticks_rotation='vertical', cmap='viridis')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Optionally, display top N misclassifications\n",
        "    if top_n_classes < len(class_names):\n",
        "        misclassifications = np.argsort(-cm.sum(axis=1))[:top_n_classes]\n",
        "        cm_top_n = cm[misclassifications][:, misclassifications]\n",
        "        class_names_top_n = [class_names[i] for i in misclassifications]\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))  # Smaller size for top misclassifications\n",
        "        disp_top_n = ConfusionMatrixDisplay(confusion_matrix=cm_top_n, display_labels=class_names_top_n)\n",
        "        disp_top_n.plot(ax=ax, xticks_rotation='vertical', cmap='viridis', norm=LogNorm())\n",
        "        plt.title(f'Top {top_n_classes} Misclassifications')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Print classification report\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "plot_results(trained_model, dataloaders['test'], class_names)"
      ],
      "metadata": {
        "id": "eUYuoxZKnFKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explainability and Interpretability using Grad-CAM\n",
        "def apply_gradcam(model, dataloader, target_layer, class_names):\n",
        "    model.eval()\n",
        "    # Ensure the model's device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialize GradCAM without 'use_cuda'\n",
        "    grad_cam = GradCAM(model=model, target_layers=[target_layer])\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        for i in range(len(inputs)):\n",
        "            input_img = inputs[i].cpu().numpy().transpose((1, 2, 0))\n",
        "            input_img = (input_img - input_img.min()) / (input_img.max() - input_img.min())\n",
        "\n",
        "            # Get the GradCAM result\n",
        "            grayscale_cam = grad_cam(input_tensor=inputs[i].unsqueeze(0))\n",
        "            visualization = show_cam_on_image(input_img, grayscale_cam[0], use_rgb=True)\n",
        "\n",
        "            plt.imshow(visualization)\n",
        "            plt.title(f\"Predicted: {class_names[preds[i]]}, Actual: {class_names[labels[i]]}\")\n",
        "            plt.show()\n",
        "\n",
        "# Ensure that 'model.classifier' points to the correct layer\n",
        "target_layer = trained_model.classifier\n",
        "apply_gradcam(trained_model, dataloaders['test'], target_layer, class_names)"
      ],
      "metadata": {
        "id": "rRGok7wsylOr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}